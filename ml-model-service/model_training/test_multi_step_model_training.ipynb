{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ca5070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07685a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame count: 2356\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file from the model_training folder\n",
    "df = pd.read_csv(\"../model_training/yamanadata.tsv\",sep=\"\\t\")\n",
    "print(f\"Initial DataFrame count: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "765d95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of selected types for the second-level models\n",
    "dark_pattern_types = [\"Fake Scarcity\", \"Fake Social Proof\", \"Fake Urgency\", \"Misdirection\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713e69f",
   "metadata": {},
   "source": [
    "# Step 1: First-level model to distinguish between Dark Patterns and Not Dark Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f0ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boolean mask\n",
    "dark_pattern_mask = df['Pattern Category'].isin(dark_pattern_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e673cf",
   "metadata": {},
   "source": [
    "## Assign labels for the first-level model (0 for Not Dark Patterns, 1 for Dark Patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "212b021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['First_Level_Label'] = dark_pattern_mask.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a985eb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame count: 1884\n",
      "Test DataFrame count: 472\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets for the first-level model\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train DataFrame count: {len(train_df)}\")\n",
    "print(f\"Test DataFrame count: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec2b5e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipelines for the first-level models\n",
    "first_level_algorithms = {\n",
    "    \"Multinomial Naive Bayes\": make_pipeline(TfidfVectorizer(), MultinomialNB()),\n",
    "    \"Support Vector Machines\": make_pipeline(TfidfVectorizer(), SVC(kernel='linear')),\n",
    "    \"Random Forest\": make_pipeline(TfidfVectorizer(), RandomForestClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700ca5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for trained models if it doesn't exist\n",
    "trained_models_folder = \"trained_models\"\n",
    "os.makedirs(trained_models_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4152b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_level_models_folder = os.path.join(trained_models_folder, \"first_level_models\")\n",
    "os.makedirs(first_level_models_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "727a9372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes First-Level Model Accuracy: 0.82\n",
      "Support Vector Machines First-Level Model Accuracy: 0.93\n",
      "Random Forest First-Level Model Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "for algo_name, model in first_level_algorithms.items():\n",
    "    train_df = train_df.dropna(subset=['text'])\n",
    "    train_df['text'] = train_df['text'].astype(str)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    model.fit(train_df['text'], train_df['label'])\n",
    "    predictions = model.predict(test_df['text'])\n",
    "    accuracy = accuracy_score(test_df['label'], predictions)\n",
    "    model_path = os.path.join(first_level_models_folder, f'{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "    dump(model, model_path)\n",
    "    print(f\"{algo_name} First-Level Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d075a1d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "testdf = pd.read_csv(\"../model_training/travel_test.csv\")\n",
    "testpredict = model.predict(testdf['Text'])\n",
    "print(testpredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32bf5d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial Naive Bayes First-Level Model Accuracy: 0.74\n",
      "Support Vector Machines First-Level Model Accuracy: 0.71\n",
      "Random Forest First-Level Model Accuracy: 0.74\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate each first-level model\n",
    "for algo_name, model in first_level_algorithms.items():\n",
    "    \n",
    "    # Drop rows with missing values in the 'Text' column\n",
    "    train_df = train_df.dropna(subset=['Text'])\n",
    "\n",
    "    # Ensure 'Text' column has string data type\n",
    "    train_df['Text'] = train_df['Text'].astype(str)\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "    # Fit the first-level model\n",
    "    model.fit(train_df['Text'], train_df['First_Level_Label'])\n",
    "\n",
    "    # Evaluate the first-level model on the test set\n",
    "    predictions = model.predict(test_df['Text'])\n",
    "    accuracy = accuracy_score(test_df['First_Level_Label'], predictions)\n",
    "\n",
    "    # Save the trained first-level model to a file in the first_level_models folder\n",
    "    model_path = os.path.join(first_level_models_folder, f'{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "    dump(model, model_path)\n",
    "\n",
    "    print(f\"{algo_name} First-Level Model Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea69d778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text to predict: Reviewed by group\n",
      "\n",
      "First-Level Predictions:\n",
      "Multinomial Naive Bayes: 1\n",
      "Support Vector Machines: 1\n",
      "Random Forest: 1\n"
     ]
    }
   ],
   "source": [
    "# User input text\n",
    "user_input_text = \"Reviewed by group\"\n",
    "print(f\"\\nText to predict: {user_input_text}\")\n",
    "\n",
    "# Load and make predictions with the first-level models\n",
    "first_level_predictions = {}\n",
    "\n",
    "for algo_name, model in first_level_algorithms.items():\n",
    "    # Load the saved model\n",
    "    model_path = os.path.join(first_level_models_folder, f'{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "    loaded_model = load(model_path)\n",
    "\n",
    "    # Make predictions\n",
    "    first_level_prediction = loaded_model.predict([user_input_text])\n",
    "    \n",
    "    # first_level_prediction[0]: Extracts the actual prediction value from the array.\n",
    "    first_level_predictions[algo_name] = first_level_prediction[0]\n",
    "\n",
    "# Print first-level predictions\n",
    "print(\"\\nFirst-Level Predictions:\")\n",
    "for algo_name, prediction in first_level_predictions.items():\n",
    "    print(f\"{algo_name}: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f6ab14",
   "metadata": {},
   "source": [
    "# Step 2: Train second-level models for each Dark Pattern type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cf5e33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dark Pattern containing dataset: 153\n"
     ]
    }
   ],
   "source": [
    "# Filter the original DataFrame for Dark Pattern types\n",
    "second_level_df = df[df['Type'].isin(dark_pattern_types)]\n",
    "print(f\"Total Dark Pattern containing dataset: {len(second_level_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b7b7d128",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_scarcity_df = second_level_df[second_level_df['Type'] == 'Fake Scarcity']\n",
    "fake_social_proof_df = second_level_df[second_level_df['Type'] == 'Fake Social Proof']\n",
    "fake_urgency_df = second_level_df[second_level_df['Type'] == 'Fake Urgency']\n",
    "misdirection_df = second_level_df[second_level_df['Type'] == 'Misdirection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b1006d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fake Scarcity DataFrame Length: 28\n",
      "Fake Social Proof DataFrame Length: 24\n",
      "Fake Urgency DataFrame Length: 37\n",
      "Misdirection DataFrame Length: 64\n"
     ]
    }
   ],
   "source": [
    "# Print the length of each DataFrame\n",
    "print(f\"Fake Scarcity DataFrame Length: {len(fake_scarcity_df)}\")\n",
    "print(f\"Fake Social Proof DataFrame Length: {len(fake_social_proof_df)}\")\n",
    "print(f\"Fake Urgency DataFrame Length: {len(fake_urgency_df)}\")\n",
    "print(f\"Misdirection DataFrame Length: {len(misdirection_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "425f70fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder for trained second-level models if it doesn't exist\n",
    "second_level_models_folder = os.path.join(trained_models_folder, \"second_level_models\")\n",
    "os.makedirs(second_level_models_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68706472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Second-Level Models for Fake Scarcity:\n",
      "Positive Dataset Length: 28\n",
      "Negative Dataset Length: 28\n",
      "Positive Train Dataset Length: 22\n",
      "Negative Train Dataset Length: 22\n",
      "Positive Test Dataset Length: 6\n",
      "Negative Test Dataset Length: 6\n",
      "Fake Scarcity - Multinomial Naive Bayes Second-Level Model Accuracy (Positive): 1.00\n",
      "Fake Scarcity - Multinomial Naive Bayes Second-Level Model Accuracy (Negative): 1.00\n",
      "Fake Scarcity - Support Vector Machines Second-Level Model Accuracy (Positive): 1.00\n",
      "Fake Scarcity - Support Vector Machines Second-Level Model Accuracy (Negative): 1.00\n",
      "Fake Scarcity - Random Forest Second-Level Model Accuracy (Positive): 1.00\n",
      "Fake Scarcity - Random Forest Second-Level Model Accuracy (Negative): 1.00\n",
      "\n",
      "Training Second-Level Models for Fake Social Proof:\n",
      "Positive Dataset Length: 24\n",
      "Negative Dataset Length: 24\n",
      "Positive Train Dataset Length: 19\n",
      "Negative Train Dataset Length: 19\n",
      "Positive Test Dataset Length: 5\n",
      "Negative Test Dataset Length: 5\n",
      "Fake Social Proof - Multinomial Naive Bayes Second-Level Model Accuracy (Positive): 1.00\n",
      "Fake Social Proof - Multinomial Naive Bayes Second-Level Model Accuracy (Negative): 1.00\n",
      "Fake Social Proof - Support Vector Machines Second-Level Model Accuracy (Positive): 1.00\n",
      "Fake Social Proof - Support Vector Machines Second-Level Model Accuracy (Negative): 1.00\n",
      "Fake Social Proof - Random Forest Second-Level Model Accuracy (Positive): 1.00\n",
      "Fake Social Proof - Random Forest Second-Level Model Accuracy (Negative): 1.00\n",
      "\n",
      "Training Second-Level Models for Fake Urgency:\n",
      "Positive Dataset Length: 37\n",
      "Negative Dataset Length: 37\n",
      "Positive Train Dataset Length: 29\n",
      "Negative Train Dataset Length: 29\n",
      "Positive Test Dataset Length: 8\n",
      "Negative Test Dataset Length: 8\n",
      "Fake Urgency - Multinomial Naive Bayes Second-Level Model Accuracy (Positive): 1.00\n",
      "Fake Urgency - Multinomial Naive Bayes Second-Level Model Accuracy (Negative): 1.00\n",
      "Fake Urgency - Support Vector Machines Second-Level Model Accuracy (Positive): 1.00\n",
      "Fake Urgency - Support Vector Machines Second-Level Model Accuracy (Negative): 1.00\n",
      "Fake Urgency - Random Forest Second-Level Model Accuracy (Positive): 1.00\n",
      "Fake Urgency - Random Forest Second-Level Model Accuracy (Negative): 1.00\n",
      "\n",
      "Training Second-Level Models for Misdirection:\n",
      "Positive Dataset Length: 64\n",
      "Negative Dataset Length: 64\n",
      "Positive Train Dataset Length: 51\n",
      "Negative Train Dataset Length: 51\n",
      "Positive Test Dataset Length: 13\n",
      "Negative Test Dataset Length: 13\n",
      "Misdirection - Multinomial Naive Bayes Second-Level Model Accuracy (Positive): 1.00\n",
      "Misdirection - Multinomial Naive Bayes Second-Level Model Accuracy (Negative): 1.00\n",
      "Misdirection - Support Vector Machines Second-Level Model Accuracy (Positive): 1.00\n",
      "Misdirection - Support Vector Machines Second-Level Model Accuracy (Negative): 1.00\n",
      "Misdirection - Random Forest Second-Level Model Accuracy (Positive): 1.00\n",
      "Misdirection - Random Forest Second-Level Model Accuracy (Negative): 1.00\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each Dark Pattern type\n",
    "for dark_pattern_type in dark_pattern_types:\n",
    "    print(f\"\\nTraining Second-Level Models for {dark_pattern_type}:\")\n",
    "\n",
    "    # Filter the dataset for the specific Dark Pattern type\n",
    "    positive_df = second_level_df[second_level_df['Type'] == dark_pattern_type]\n",
    "\n",
    "    # Create a combined negative dataset\n",
    "    negative_df = second_level_df[second_level_df['Type'] != dark_pattern_type]\n",
    "\n",
    "    # Ensure that negative_df contains instances with First_Level_Label as 0\n",
    "    negative_df.loc[:, 'First_Level_Label'] = 0\n",
    "\n",
    "    # Ensure that the negative dataset has the same number of instances as the positive dataset\n",
    "    negative_df = negative_df.sample(n=len(positive_df), random_state=42)\n",
    "\n",
    "    # Combine positive and negative datasets\n",
    "    train_combined_df = pd.concat([positive_df, negative_df])\n",
    "\n",
    "    # Shuffle the combined training dataset\n",
    "    train_combined_df = train_combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    # Display the lengths of positive and negative datasets for verification\n",
    "    print(f\"Positive Dataset Length: {len(positive_df)}\")\n",
    "    print(f\"Negative Dataset Length: {len(negative_df)}\")\n",
    "\n",
    "    # Split the datasets into training and testing sets for the second-level model\n",
    "    train_pos_df, test_pos_df = train_test_split(positive_df, test_size=0.2, random_state=42)\n",
    "    train_neg_df, test_neg_df = train_test_split(negative_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Display the lengths of positive and negative datasets for verification\n",
    "    print(f\"Positive Train Dataset Length: {len(train_pos_df)}\")\n",
    "    print(f\"Negative Train Dataset Length: {len(train_neg_df)}\")\n",
    "    print(f\"Positive Test Dataset Length: {len(test_pos_df)}\")\n",
    "    print(f\"Negative Test Dataset Length: {len(test_neg_df)}\")\n",
    "\n",
    "    # Create pipelines for the second-level models\n",
    "    second_level_algorithms = {\n",
    "        \"Multinomial Naive Bayes\": make_pipeline(TfidfVectorizer(), MultinomialNB()),\n",
    "        \"Support Vector Machines\": make_pipeline(TfidfVectorizer(), SVC(kernel='linear')),\n",
    "        \"Random Forest\": make_pipeline(TfidfVectorizer(), RandomForestClassifier())\n",
    "    }\n",
    "\n",
    "    # Train and evaluate each second-level model\n",
    "    for algo_name, model in second_level_algorithms.items():\n",
    "        # Drop rows with missing values in the 'Text' column\n",
    "        train_combined_df = train_combined_df.dropna(subset=['Text'])\n",
    "\n",
    "        # Ensure 'Text' column has string data type\n",
    "        train_combined_df['Text'] = train_combined_df['Text'].astype(str)\n",
    "\n",
    "        # Reset the index of the DataFrame\n",
    "        train_combined_df = train_combined_df.reset_index(drop=True)\n",
    "\n",
    "        # Fit the second-level model\n",
    "        model.fit(train_combined_df['Text'], train_combined_df['First_Level_Label'])\n",
    "\n",
    "        # Evaluate the second-level model on the test set (positive and negative combined)\n",
    "        predictions_pos = model.predict(test_pos_df['Text'])\n",
    "        predictions_neg = model.predict(test_neg_df['Text'])\n",
    "\n",
    "        # Calculate accuracy separately for positive and negative datasets\n",
    "        accuracy_pos = accuracy_score(test_pos_df['First_Level_Label'], predictions_pos)\n",
    "        accuracy_neg = accuracy_score(test_neg_df['First_Level_Label'], predictions_neg)\n",
    "\n",
    "        # Save the trained second-level model to a file in the second_level_models folder\n",
    "        model_path = os.path.join(second_level_models_folder, f'{dark_pattern_type.lower().replace(\" \", \"_\")}_{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "        dump(model, model_path)\n",
    "\n",
    "        print(f\"{dark_pattern_type} - {algo_name} Second-Level Model Accuracy (Positive): {accuracy_pos:.2f}\")\n",
    "        print(f\"{dark_pattern_type} - {algo_name} Second-Level Model Accuracy (Negative): {accuracy_neg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5ef174d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input Text: Recommended by group\n",
      "\n",
      "First-Level Predictions:\n",
      "Multinomial Naive Bayes: Dark Pattern\n",
      "Support Vector Machines: Dark Pattern\n",
      "Random Forest: Dark Pattern\n",
      "\n",
      "Second-Level Predictions:\n",
      "\n",
      "Testing for Fake Scarcity:\n",
      "Multinomial Naive Bayes: No\n",
      "Support Vector Machines: No\n",
      "Random Forest: No\n",
      "\n",
      "Testing for Fake Social Proof:\n",
      "Multinomial Naive Bayes: Yes\n",
      "Support Vector Machines: Yes\n",
      "Random Forest: Yes\n",
      "\n",
      "Testing for Fake Urgency:\n",
      "Multinomial Naive Bayes: No\n",
      "Support Vector Machines: No\n",
      "Random Forest: No\n",
      "\n",
      "Testing for Misdirection:\n",
      "Multinomial Naive Bayes: No\n",
      "Support Vector Machines: No\n",
      "Random Forest: No\n"
     ]
    }
   ],
   "source": [
    "def predict_dark_pattern(input_text):\n",
    "    print(f\"\\nInput Text: {input_text}\")\n",
    "\n",
    "    # Load and make predictions with the first-level models\n",
    "    first_level_predictions = {}\n",
    "\n",
    "    print(\"\\nFirst-Level Predictions:\")\n",
    "    for algo_name, model in first_level_algorithms.items():\n",
    "        # Load the saved model\n",
    "        model_path = os.path.join(first_level_models_folder, f'{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "        loaded_model = load(model_path)\n",
    "\n",
    "        # Make predictions\n",
    "        first_level_prediction = loaded_model.predict([input_text])\n",
    "\n",
    "        # first_level_prediction[0]: Extracts the actual prediction value from the array.\n",
    "        first_level_predictions[algo_name] = first_level_prediction[0]\n",
    "\n",
    "        print(f\"{algo_name}: {'Dark Pattern' if first_level_prediction[0] == 1 else 'Not Dark Pattern'}\")\n",
    "\n",
    "    # Check if it is identified as a Dark Pattern\n",
    "    if any(value == 1 for value in first_level_predictions.values()):\n",
    "        # Load and make predictions with the second-level models\n",
    "        print(\"\\nSecond-Level Predictions:\")\n",
    "        for dark_pattern_type in dark_pattern_types:\n",
    "            print(f\"\\nTesting for {dark_pattern_type}:\")\n",
    "\n",
    "            # Load the saved second-level model\n",
    "            for algo_name in second_level_algorithms.keys():\n",
    "                model_path = os.path.join(second_level_models_folder, f'{dark_pattern_type.lower().replace(\" \", \"_\")}_{algo_name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "                loaded_model = load(model_path)\n",
    "\n",
    "                # Make predictions\n",
    "                second_level_prediction = loaded_model.predict([input_text])\n",
    "\n",
    "                # Map numerical labels to Dark Pattern names\n",
    "                dark_pattern_names = {\n",
    "                    0: f\"No\",\n",
    "                    1: f\"Yes\"\n",
    "                }\n",
    "\n",
    "                # Extract the actual prediction value from the array\n",
    "                second_level_prediction_name = dark_pattern_names.get(second_level_prediction[0], \"Unknown\")\n",
    "\n",
    "                print(f\"{algo_name}: {second_level_prediction_name}\")\n",
    "\n",
    "# Example usage\n",
    "user_input_text = \"Recommended by group\"\n",
    "predict_dark_pattern(user_input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2797b3fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
